#!/usr/bin/env python3
"""LottoGoGo ML - XGBoost ê¸°ë°˜ ë¡œë˜ ë²ˆí˜¸ ì¶”ì²œê¸°"""

import json
import sys
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd

from lottogogo.data.loader import LottoHistoryLoader
from lottogogo.tuning.xgb_ranker import XGBRanker
from lottogogo.tuning.feature_builder import FeatureBuilder
from lottogogo.engine.score.normalizer import ProbabilityNormalizer
from lottogogo.engine.sampler.monte_carlo import MonteCarloSampler
from lottogogo.engine.filters import (
    FilterPipeline,
    SumFilter,
    ACFilter,
    ZoneFilter,
    TailFilter,
    OddEvenFilter,
    HighLowFilter,
    HistoryFilter,
)
from lottogogo.engine.ranker.scorer import CombinationRanker
from lottogogo.engine.ranker.diversity import DiversitySelector

NUMBER_COLS = ["n1", "n2", "n3", "n4", "n5", "n6"]

# ============================================================
# ğŸ° ë„ë°•ì‚¬ì˜ ì˜¤ë¥˜ í•„í„°ë§ ì„¤ì • (recommend.pyì™€ ë™ì¼)
# ============================================================
RARE_PAIR_COMBINATIONS = [
    (8, 12), (8, 26), (24, 43), (26, 32), (1, 22), (3, 25),
    (4, 30), (6, 33), (8, 9), (11, 40), (23, 41), (6, 23),
    (6, 29), (7, 32), (9, 20), (11, 34), (16, 22), (19, 29),
    (24, 28), (25, 41), (29, 30), (37, 44),
]

EXCLUDE_NUMBERS = {8}
MAX_CARRYOVER_IN_COMBO = 2


def contains_rare_pair(combination: tuple[int, ...]) -> bool:
    """ì¡°í•©ì´ í¬ê·€ ìŒì„ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸"""
    nums = set(combination)
    for a, b in RARE_PAIR_COMBINATIONS:
        if a in nums and b in nums:
            return True
    return False


def contains_excluded_number(combination: tuple[int, ...]) -> bool:
    """ì¡°í•©ì´ ì œì™¸ ìˆ«ìë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸"""
    return bool(set(combination) & EXCLUDE_NUMBERS)


def exceeds_carryover_limit(combination: tuple[int, ...], carryover_numbers: set[int]) -> bool:
    """ì¡°í•©ì´ ì´ì›”ìˆ˜ ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸"""
    carryover_count = len(set(combination) & carryover_numbers)
    return carryover_count > MAX_CARRYOVER_IN_COMBO


def save_model(model, latest_round: int, weights_path: str | None = None, model_dir: str = "data") -> None:
    """XGBoost ëª¨ë¸ì„ ì €ì¥í•˜ê³  ë©”íƒ€ë°ì´í„° ê¸°ë¡"""
    import pickle
    
    model_path = Path(model_dir)
    model_path.mkdir(parents=True, exist_ok=True)
    
    # ëª¨ë¸ ì €ì¥ (pickle ì‚¬ìš©)
    model_file = model_path / "xgb_model.pkl"
    with open(model_file, "wb") as f:
        pickle.dump(model, f)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥ (ê°€ì¤‘ì¹˜ ê²½ë¡œ í¬í•¨)
    metadata = {
        "trained_until_round": latest_round,
        "trained_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "model_file": str(model_file),
        "weights_path": weights_path,
    }
    metadata_file = model_path / "xgb_model_metadata.json"
    metadata_file.write_text(json.dumps(metadata, indent=2, ensure_ascii=False))


def load_model_if_valid(latest_round: int, weights_path: str | None = None, model_dir: str = "data"):
    """ì €ì¥ëœ ëª¨ë¸ì´ ìœ íš¨í•˜ë©´ ë¡œë“œ, ì•„ë‹ˆë©´ None ë°˜í™˜
    
    Args:
        latest_round: ìµœì‹  íšŒì°¨
        weights_path: ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê°€ì¤‘ì¹˜)
        model_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    import pickle
    
    model_path = Path(model_dir)
    metadata_file = model_path / "xgb_model_metadata.json"
    model_file = model_path / "xgb_model.pkl"
    
    # ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìœ¼ë©´ None
    if not metadata_file.exists() or not model_file.exists():
        return None
    
    try:
        metadata = json.loads(metadata_file.read_text())
        trained_round = metadata.get("trained_until_round", 0)
        trained_weights = metadata.get("weights_path", None)
        
        # ì €ì¥ëœ ëª¨ë¸ì´ ìµœì‹  ë°ì´í„°ë¡œ í•™ìŠµë˜ì—ˆê³ , ê°™ì€ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í–ˆëŠ”ì§€ í™•ì¸
        if trained_round >= latest_round and trained_weights == weights_path:
            with open(model_file, "rb") as f:
                model = pickle.load(f)
            return model, metadata
        else:
            return None
    except Exception:
        return None


def main(csv_path: str = "history.csv", num_games: int = 5, seed: int | None = None, weights_path: str | None = None):
    """XGBoost ê¸°ë°˜ ë¡œë˜ ë²ˆí˜¸ ì¶”ì²œ ì‹¤í–‰"""
    
    # ì‹œë“œ ì„¤ì •
    if seed is None:
        seed = int(datetime.now().timestamp()) % 100000
    
    print("=" * 60)
    print("ğŸ¤– LottoGoGo ML - XGBoost ê¸°ë°˜ ë¡œë˜ ë²ˆí˜¸ ì¶”ì²œê¸°")
    print("=" * 60)
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ² Seed: {seed}")
    print()

    # 1. ë°ì´í„° ë¡œë“œ
    loader = LottoHistoryLoader()
    try:
        history = loader.load_and_validate(csv_path).reset_index(drop=True)
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        sys.exit(1)
    
    latest_round = int(history["round"].max())
    print(f"ğŸ“Š ë°ì´í„°: {len(history)}íšŒì°¨ (ìµœì‹ : {latest_round}íšŒ)")

    # 1.1 ìµœì í™” ê°€ì¤‘ì¹˜ ë¡œë“œ
    weights = {}
    if weights_path:
        w_path = Path(weights_path)
        if w_path.exists():
            print(f"ğŸ“‚ ê°€ì¤‘ì¹˜ ë¡œë“œ: {weights_path}")
            try:
                weights_data = json.loads(w_path.read_text(encoding="utf-8"))
                weights = weights_data.get("weights", {})
                print(f"   - Cycle: {weights_data.get('cycle_label', 'unknown')}")
            except Exception as e:
                print(f"âš ï¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨ (ê¸°ë³¸ê°’ ì‚¬ìš©): {e}")
        else:
            print(f"âš ï¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {weights_path} (ê¸°ë³¸ê°’ ì‚¬ìš©)")

    # ê³¼ê±° ë‹¹ì²¨ë²ˆí˜¸ ì¶”ì¶œ
    historical_draws = [
        tuple(row[NUMBER_COLS].astype(int).tolist()) 
        for _, row in history.iterrows()
    ]

    # 2. XGBoost ëª¨ë¸ í•™ìŠµ ë˜ëŠ” ë¡œë“œ
    print("\nğŸ§  XGBoost ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
    
    # ì €ì¥ëœ ëª¨ë¸ í™•ì¸ (ê°€ì¤‘ì¹˜ ê²½ë¡œ í¬í•¨)
    cached = load_model_if_valid(latest_round, weights_path)
    
    if cached is not None:
        model, metadata = cached
        trained_round = metadata["trained_until_round"]
        trained_at = metadata["trained_at"]
        print(f"   âœ… ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ (í•™ìŠµ íšŒì°¨: {trained_round}íšŒ, í•™ìŠµ ì‹œê°: {trained_at})")
        print(f"   âš¡ ì¬í•™ìŠµ ìƒëµ (ìµœì‹  ë°ì´í„°ì™€ ì¼ì¹˜)")
    else:
        # ëª¨ë¸ì´ ì—†ê±°ë‚˜ ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ìƒˆë¡œ í•™ìŠµ
        if Path("data/xgb_model_metadata.json").exists():
            old_metadata = json.loads(Path("data/xgb_model_metadata.json").read_text())
            old_round = old_metadata.get("trained_until_round", 0)
            print(f"   ğŸ”„ ìƒˆë¡œìš´ íšŒì°¨ ê°ì§€ ({old_round}íšŒ â†’ {latest_round}íšŒ)")
        print(f"   ğŸ§  ëª¨ë¸ í•™ìŠµ ì¤‘... (ì „ì²´ {latest_round}íšŒì°¨ ë°ì´í„°)")
        
        builder = FeatureBuilder(history, weights=weights if weights else None)
        ranker = XGBRanker(builder)
        
        # ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ (ê°€ì¤‘ì¹˜ ì ìš©)
        X_train, y_train = builder.build((1, latest_round))
        
        spw = FeatureBuilder.scale_pos_weight(y_train)
        xgb_params = {
            "objective": "binary:logistic",
            "eval_metric": "logloss",
            "max_depth": 6,
            "learning_rate": 0.1,
            "n_estimators": 200,
            "scale_pos_weight": spw,
            "seed": seed,
            "verbosity": 0,
        }
        
        import xgboost as xgb
        model = xgb.XGBClassifier(
            n_estimators=xgb_params.pop("n_estimators"),
            random_state=xgb_params.pop("seed"),
            **xgb_params,
        )
        model.fit(X_train, y_train)
        
        # ëª¨ë¸ ì €ì¥ (ê°€ì¤‘ì¹˜ ê²½ë¡œ í¬í•¨)
        save_model(model, latest_round, weights_path)
        print(f"   âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ (data/xgb_model.pkl)")

    # 3. ë‹¤ìŒ íšŒì°¨ ì˜ˆì¸¡
    print(f"\nğŸ”® {latest_round + 1}íšŒ ë²ˆí˜¸ í™•ë¥  ì˜ˆì¸¡ ì¤‘...")
    next_round = latest_round + 1
    
    # FeatureBuilder ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìºì‹œëœ ëª¨ë¸ ì‚¬ìš© ì‹œì—ë„ í•„ìš”)
    if cached is not None:
        builder = FeatureBuilder(history, weights=weights if weights else None)
    
    # íŠ¹ì§• ì¶”ì¶œ (ê°€ì¤‘ì¹˜ ì ìš©)
    features = builder._extract_features(history, next_round, weights=weights if weights else None)
    probs_raw = model.predict_proba(features)[:, 1]
    
    # í™•ë¥ ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    probs = {n: float(probs_raw[n - 1]) for n in range(1, 46)}
    
    # í™•ë¥  ì •ê·œí™” (ìƒ˜í”Œë§ìš©)
    probs_normalized = ProbabilityNormalizer.to_sampling_probabilities(
        probs, 
        temperature=0.5, 
        min_prob_floor=0.005
    )
    
    # ìƒìœ„ ë²ˆí˜¸ ì¶œë ¥
    sorted_numbers = sorted(range(1, 46), key=lambda n: probs[n], reverse=True)
    print(f"\nğŸ“Š ì˜ˆì¸¡ í™•ë¥  ìƒìœ„ 15ê°œ:")
    print(f"   {'ë²ˆí˜¸':>4} | {'ì˜ˆì¸¡ í™•ë¥ ':>8} | {'ìƒ˜í”Œë§ í™•ë¥ ':>10}")
    print(f"   {'-'*4}-+-{'-'*8}-+-{'-'*10}")
    for num in sorted_numbers[:15]:
        print(f"   {num:4d} | {probs[num]:8.4f} | {probs_normalized[num]:9.2%}")

    # Carryover ë²ˆí˜¸ ì¶”ì¶œ (ì§ì „ + 2ì£¼ì „)
    last_round = history[history["round"] == latest_round].iloc[0]
    carryover_numbers = set(int(last_round[col]) for col in NUMBER_COLS)
    
    if len(history) >= 2:
        second_last_round = history[history["round"] == latest_round - 1].iloc[0]
        carryover2_numbers = set(int(second_last_round[col]) for col in NUMBER_COLS)
    else:
        carryover2_numbers = set()
    
    all_carryover_numbers = carryover_numbers | carryover2_numbers
    print(f"\n   ğŸ”„ Carryover (ì§ì „): {sorted(carryover_numbers)}")
    print(f"   ğŸ”„ Carryover2 (2ì£¼ì „): {sorted(carryover2_numbers)}")
    print(f"   â†’ ì´ì›”ìˆ˜(ì§ì „+2ì£¼ì „) ìµœëŒ€ {MAX_CARRYOVER_IN_COMBO}ê°œê¹Œì§€ í—ˆìš©")

    # 4. ì¡°í•© ìƒì„±
    print(f"\nğŸ² ì¡°í•© ìƒì„± ì¤‘ (100,000ê°œ)...")
    sampler = MonteCarloSampler(sample_size=100000, chunk_size=20000)
    combinations = sampler.sample(probs_normalized, seed=seed)

    # 5. í•„í„°ë§
    print("ğŸ” í•„í„°ë§ ì¤‘...")
    pipeline = FilterPipeline([
        SumFilter(min_sum=100, max_sum=175),
        ACFilter(min_ac=7),
        ZoneFilter(max_per_zone=3),
        TailFilter(max_same_tail=2),
        OddEvenFilter(min_odd=2, max_odd=4),
        HighLowFilter(min_high=2, max_high=4),
        HistoryFilter(historical_draws=historical_draws, match_threshold=5),
    ])
    filtered = pipeline.filter_combinations(combinations)
    pass_rate = len(filtered) / len(combinations) * 100
    print(f"   ê¸°ë³¸ í•„í„° í†µê³¼: {len(filtered):,}ê°œ ({pass_rate:.1f}%)")

    # ğŸ° ë„ë°•ì‚¬ì˜ ì˜¤ë¥˜ í•„í„°ë§
    print("\nğŸ° ë„ë°•ì‚¬ì˜ ì˜¤ë¥˜ í•„í„°ë§...")
    before_gambler = len(filtered)
    filtered = [
        combo for combo in filtered
        if not contains_rare_pair(combo) 
        and not contains_excluded_number(combo)
        and not exceeds_carryover_limit(combo, all_carryover_numbers)
    ]
    after_gambler = len(filtered)
    removed = before_gambler - after_gambler
    print(f"   ì œì™¸ëœ ì¡°í•©: {removed:,}ê°œ")
    print(f"   - í¬ê·€ ìŒ ì¡°í•©: {len(RARE_PAIR_COMBINATIONS)}ê°œ íŒ¨í„´ ì œì™¸")
    print(f"   - ì œì™¸ ìˆ«ì: {EXCLUDE_NUMBERS}")
    print(f"   - ì´ì›”ìˆ˜ ì œí•œ: ìµœëŒ€ {MAX_CARRYOVER_IN_COMBO}ê°œ (ì§ì „+2ì£¼ì „ í•©ê³„)")
    print(f"   ìµœì¢… í†µê³¼: {len(filtered):,}ê°œ")

    # 6. ë­í‚¹ ë° ë‹¤ì–‘ì„± ì ìš©
    print("ğŸ† ë­í‚¹ ë° ë‹¤ì–‘ì„± ì ìš©...")
    
    # ì¤‘ë³µ ì œê±°
    unique_filtered = list(dict.fromkeys(filtered))
    if len(unique_filtered) != len(filtered):
        print(f"   (ì¤‘ë³µ ì œê±°) {len(filtered):,} -> {len(unique_filtered):,}ê°œ")
    filtered = unique_filtered

    ranker_obj = CombinationRanker()
    ranked = ranker_obj.rank(filtered, probs)
    candidates = [r.numbers for r in ranked]

    base_overlap = 3
    selector = DiversitySelector(max_overlap=base_overlap)
    final = selector.select(candidates, output_count=num_games)

    # ë‹¤ì–‘ì„± ì¡°ê±´ ì™„í™”
    if len(final) < num_games:
        print(f"   âš ï¸ ë‹¤ì–‘ì„± ì¡°ê±´(max_overlap={base_overlap})ìœ¼ë¡œ {len(final)}/{num_games}ê°œë§Œ ì„ íƒë¨. ì¡°ê±´ì„ ì™„í™”í•©ë‹ˆë‹¤.")
        for overlap in range(base_overlap + 1, 7):
            final = DiversitySelector(max_overlap=overlap).select(candidates, output_count=num_games)
            if len(final) >= num_games:
                print(f"   -> max_overlap={overlap}ë¡œ {len(final)}/{num_games}ê°œ ì„ íƒ")
                break

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    if len(final) < num_games:
        print(f"âš ï¸ ìš”ì²­ {num_games}ê²Œì„ ì¤‘ {len(final)}ê²Œì„ë§Œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. (í›„ë³´ ë¶€ì¡±/ì¡°ê±´ ê³¼ë‹¤)")
    print(f"ğŸ¯ {latest_round + 1}íšŒ ì¶”ì²œ ë²ˆí˜¸ ({len(final)}/{num_games}ê²Œì„)")
    print("=" * 60)
    
    for i, combo in enumerate(final, 1):
        numbers_str = ", ".join(f"{n:2d}" for n in combo)
        total = sum(combo)
        avg_prob = np.mean([probs[n] for n in combo])
        print(f"\n  {i}ê²Œì„: [{numbers_str}]")
        print(f"         í•©ê³„: {total} | í™€ìˆ˜: {sum(1 for n in combo if n % 2)}ê°œ | í‰ê·  ì˜ˆì¸¡í™•ë¥ : {avg_prob:.4f}")
    
    print("\n" + "=" * 60)
    print("ğŸ¤– XGBoost ì¶”ì²œ ì™„ë£Œ! í–‰ìš´ì„ ë¹•ë‹ˆë‹¤!")
    print("=" * 60)
    
    return final


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="LottoGoGo ML - XGBoost ë²ˆí˜¸ ì¶”ì²œê¸°")
    parser.add_argument("--csv", default="history.csv", help="CSV íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--games", type=int, default=5, help="ì¶”ì²œ ê²Œì„ ìˆ˜")
    parser.add_argument("--seed", type=int, default=None, help="ëœë¤ ì‹œë“œ")
    parser.add_argument("--weights", default=None, help="ìµœì í™” ê°€ì¤‘ì¹˜ JSON ê²½ë¡œ")
    
    args = parser.parse_args()
    main(csv_path=args.csv, num_games=args.games, seed=args.seed, weights_path=args.weights)
